{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn.ipynb","provenance":[],"mount_file_id":"185jgKG8YeQMOBNCayVa17lHfHD3Eu_8t","authorship_tag":"ABX9TyPryI68X7gKYjWFkE+BuUIt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"t4u3wcGt_Kk-"},"source":["import glob\n","import torch\n","import pathlib\n","import re\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import tqdm\n","from statistics import mean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7S-TvFRqAJl5"},"source":["remove_marks_regrex = re.compile(\"[,\\.\\(\\)\\[\\]\\*:;]|<.*?>\")\n","shift_marks_regrex = re.compile(\"([?!])\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypRxHMq4AlO7"},"source":["def text2ids(text, vocab_dict):\n","    # !?以外の記号を削除\n","    text = remove_marks_regrex.sub(\"\", text)\n","    # I?と単語の間にスペースを入れる\n","    text = shift_marks_regrex.sub(r\" \\1\", text)\n","    tokens = text.split()\n","    return [vocab_dict.get(token, 0) for token in tokens]\n","\n","def list2tensor(token_idxes, max_len=100, padding= True):\n","    if len(token_idxes) > max_len:\n","        token_idxes = token_idxes[:max_len]\n","    n_tokens = len(token_idxes)\n","    if padding:\n","        token_idxes = token_idexes + [0]*(max_len - len(token_idxes))\n","    return torch.tensor(token_idxes, dtype= torch.int64), n_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"niD54lJFCikf"},"source":["class IMDBDataset(Dataset):\n","    def __init__(self, dir_path, train=True, max_len=100, padding=True):\n","        self.max_len = max_len\n","        self.padding = padding\n","        path = pathlib.Path(dir_path)\n","        print(path)\n","\n","        vocab_path = path.joinpath('imdb.vocab')\n","\n","        # ボキャラブラリファイルを読み込み、行ごとに分割\n","        self.vocab_array =  vocab_path.open().read().strip().splitlines()\n","\n","        # 単語をキーとして、値がIDのdictを作る\n","        self.vocab_dict = {w: i+1 for (i, w) in enumerate(self.vocab_array)}\n","        if train:\n","            target_path = path.joinpath('train')\n","        else:\n","            target_path = path.joinpath('test')\n","        pos_files = stored(glob.glob(str(target_path.joinpath('pos/*.txt'))))\n","        neg_files = stored(glob.glob(str(target_path.joinpath('neg/*.txt'))))\n","\n","        # zipはlistをまとめる\n","        self.labeled_files = \\\n","            list(zip([0].len(neg_files), neg_files)) + \\\n","            list(zip([1].len(pos_files), pos_files))\n","        \n","    @property\n","    def vocab_size(self):\n","        return len(self.vacab_array)\n","    \n","    def __len__(self):\n","        return len(self.labeled_files)\n","    \n","    def __getitem__(self, idx):\n","        label, f = self.labeled_files[idx]\n","        data = open(f).read().lower()\n","        data = text2ids(data, self.vacab_dict)\n","        data, n_tokens = list2tensor(data, self.max_len, self.padding)\n","        return data, label, n_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"lOYfWvegEoXr","executionInfo":{"status":"error","timestamp":1615513664505,"user_tz":-540,"elapsed":604,"user":{"displayName":"赤司一真","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR_p-_pQTdNpn5cmsz6eJNVIMnHlPzzXgvd7VZWg=s64","userId":"08143384841224428648"}},"outputId":"71097fad-d64d-4bf3-92d9-0862259e353e"},"source":["train_data = IMDBDataset('/content/sample_data/')\n","test_data = IMDBDataset('/content/sample_data/', train=False)\n","\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=True, num_workers=4)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/sample_data\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-924e8eaca2c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-d47075367e6d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dir_path, train, max_len, padding)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# ボキャラブラリファイルを読み込み、行ごとに分割\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_array\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mvocab_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# 単語をキーとして、値がIDのdictを作る\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[0;32m-> 1208\u001b[0;31m                        opener=self._opener)\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/imdb.vocab'"]}]},{"cell_type":"code","metadata":{"id":"ohjxOJPgHda5"},"source":["class SequenceTaggingNet(nn.Module):\n","    def __init__(self, num_embeddings, embedding_dim=50, hidden_size=50, num_layers=1, dropout=0.2):\n","        super().__init__()\n","        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout= dropout)\n","        self.linear = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x, h0=None, l=None):\n","        x = self.emb(x)\n","        x, h = self.lstm(x, h0)\n","        if l is not None:\n","            x = x[list(range(len(x))), l-1, :]\n","        else:\n","            x = x[:, -1, :]\n","        x = self.linear(x)\n","        x = x.squeeze()\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6xONqG7L_mA"},"source":["def eval_net(net, data_loader, device='cpu'):\n","    net.eval()\n","    ys = []\n","    ypreds = []\n","    for x, y, l in data_loader:\n","        x = x.to(device)\n","        y = y.to(device)\n","        l = l.to(device)\n","        with torch.no_grad():\n","            y_pred = net(x, l=l)\n","            y_pred = (y_pred > 0).long()\n","            ys.append(y)\n","            ypreds.append(y_pred)\n","    ys = torch.cat(ys)\n","    ypreds = torch.cat(ypreds)\n","    acc = (ys==ypreds).float().sum() / len(ys)\n","    return acc.item()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sexd0COLNTt4"},"source":["from statistics import mean\n","\n","net = SequenceTaggingNet(train_data.vocab_size+1, num_layers=2)\n","device = 'cpu'\n","net.to(device)\n","opt = optim.Adam(net.parameters())\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","for epoch in range(10):\n","    losses = []\n","    net.train()\n","    for x, y, l in tqdm.tqdm(train_loader):\n","        x = x.to(device)\n","        y = y.to(device)\n","        l = l.to(device)\n","        y_pred = net(x, l=l)\n","        loss = loss_fn(y_pred, y.float())\n","        net.zero_grad()\n","        loss.backward()\n","        net.step()\n","        losses.append(loss.item())\n","    train_acc = eval_net(net, test_loader, device)\n","    val_acc = eval_net(net, test_loader, device)\n","    print(epoch, mean(losses), train_acc, val_acc)\n"],"execution_count":null,"outputs":[]}]}